---
# Create Spark configuration ConfigMap
# This is used by JupyterHub and other Spark-enabled services

- name: Create Spark ConfigMap
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: '{{ spark_defaults_configmap_name }}'
        namespace: '{{ spark_defaults_configmap_namespace }}'
      data:
        spark-defaults.conf: |
          spark.hadoop.fs.s3a.access.key {{ spark_s3_username | default(s3_lake_reader) }}
          spark.hadoop.fs.s3a.secret.key {{ spark_s3_password | default(s3_lake_reader_secret) }}
          spark.hadoop.fs.s3a.endpoint {{ s3_endpoint }}
          spark.hadoop.fs.s3a.endpoint.region {{ s3_region | default("us-east-1") }}
          spark.databricks.delta.schema.autoMerge.enabled true
          spark.databricks.delta.merge.repartitionBeforeWrite.enabled true
          spark.databricks.delta.constraints.allowUnenforcedNotNull.enabled true
          spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension
          spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
          spark.hadoop.fs.s3a.path.style.access true
          spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
          spark.hadoop.hive.metastore.uris {{ hive_metastore_endpoint }}
          spark.sql.warehouse.dir {{ delta_lake_path }}
          spark.driver.extraJavaOptions -Divy.cache.dir=/tmp -Divy.home=/tmp
          spark.executor.memory {{ spark_memory | default("1G") }}
          spark.driver.memory {{ spark_memory | default("1G") }}
