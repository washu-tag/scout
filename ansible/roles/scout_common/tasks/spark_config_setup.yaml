---
# Create Spark configuration ConfigMap
# This is used by JupyterHub and other Spark-enabled services
#
# Parameters:
#   spark_hive_metastore_uri: Which Hive metastore to connect to
#     - Pass hive_metastore_endpoint for write access (HL7 transformer, extractors)
#     - Pass hive_metastore_endpoint_readonly for readonly access (Jupyter)
#     - Defaults to hive_metastore_endpoint (write) for backward compatibility

- name: Create Spark ConfigMap
  kubernetes.core.k8s:
    state: present
    definition:
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: '{{ spark_defaults_configmap_name }}'
        namespace: '{{ spark_defaults_configmap_namespace }}'
      data:
        spark-defaults.conf: |
          spark.hadoop.fs.s3a.access.key {{ spark_s3_username }}
          spark.hadoop.fs.s3a.secret.key {{ spark_s3_password }}
          spark.hadoop.fs.s3a.endpoint {{ s3_endpoint }}
          spark.hadoop.fs.s3a.endpoint.region {{ s3_region | default("us-east-1") }}
          spark.databricks.delta.schema.autoMerge.enabled true
          spark.databricks.delta.merge.repartitionBeforeWrite.enabled true
          spark.databricks.delta.constraints.allowUnenforcedNotNull.enabled true
          spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension
          spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
          spark.hadoop.fs.s3a.path.style.access true
          spark.hadoop.fs.s3a.aws.credentials.provider org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
          spark.hadoop.hive.metastore.uris {{ spark_hive_metastore_uri }}
          spark.sql.warehouse.dir {{ delta_lake_path }}
          spark.driver.extraJavaOptions -Divy.cache.dir=/tmp -Divy.home=/tmp
          {% if spark_memory is defined and spark_memory != "" %}
          spark.executor.memory {{ spark_memory | default("1G") }}
          spark.driver.memory {{ spark_memory | default("1G") }}
          {% endif %}
