# vLLM Production Stack Helm Values
# Generated by Scout Ansible deployment

servingEngineSpec:
  runtimeClassName: nvidia

  modelSpec:
  - name: {{ vllm_model_name }}
    modelURL: {{ vllm_model_configs[vllm_model_name].repository }}

    # Replica Configuration
    replicaCount: 1

    # Resource Allocation
    requestCPU: {{ vllm_cpu_request }}
    requestMemory: {{ vllm_memory_request }}
    requestGPU: {{ vllm_gpu_count }}
    requestGPUType: {{ vllm_gpu_type }}

    limitCPU: {{ vllm_cpu_limit }}
    limitMemory: {{ vllm_memory_limit }}

    # Storage for Model Weights
    pvcStorage: {{ vllm_storage_size }}
    pvcAccessMode:
      - {{ vllm_pvc_access_mode }}
{% if vllm_storage_class %}
    storageClass: {{ vllm_storage_class }}
{% endif %}

    # vLLM Engine Configuration
    vllmConfig:
      # Context and Model
      maxModelLen: {{ vllm_model_configs[vllm_model_name].max_model_len }}
      dtype: {{ vllm_dtype }}

      # Quantization
{% if vllm_model_configs[vllm_model_name].quantization is defined %}
      quantization: {{ vllm_model_configs[vllm_model_name].quantization }}
{% endif %}

      # Memory Optimization
      gpuMemoryUtilization: {{ vllm_gpu_memory_utilization }}
      enablePrefixCaching: {{ vllm_enable_prefix_caching | lower }}
      enableChunkedPrefill: {{ vllm_enable_chunked_prefill | lower }}
      kvCacheDtype: {{ vllm_kv_cache_dtype }}

      # Parallelism
      tensorParallelSize: {{ vllm_tensor_parallel_size }}

      # Concurrency
      maxNumSeqs: {{ vllm_model_configs[vllm_model_name].max_num_seqs }}

      # Tool Support
      enableTool: true
      toolCallParser: {{ vllm_model_configs[vllm_model_name].tool_call_parser }}
      enableAutoToolChoice: true

      # Additional Arguments
      extraArgs:
        - --disable-log-requests
        - --trust-remote-code

    # Shared Memory (critical for multi-GPU)
    shmSize: {{ vllm_shm_size }}

    # GPU Node Affinity (reuse Ollama pattern)
{% if llm_min_gpu_memory_gb | int > 0 %}
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: nvidia.com/gpu.memory
                  operator: Gt
                  values:
                    - '{{ llm_min_gpu_memory_gb }}'
{% endif %}

# Router Configuration
routerSpec:
{% if vllm_router_enabled %}
  enableRouter: true
  replicaCount: {{ vllm_router_replicas }}
  serviceType: {{ vllm_service_type }}

  # Kubernetes Service Discovery
  serviceDiscovery: k8s
  k8sServiceDiscoveryType: pod-ip

  # Routing Strategy
  routingLogic: {{ vllm_routing_logic }}

  # Metrics Collection
  engineScrapeInterval: 30
  requestStatsWindow: 60
{% else %}
  enableRouter: false
{% endif %}

# Observability (delegate to Scout's monitoring)
observabilitySpec:
  enabled: false
