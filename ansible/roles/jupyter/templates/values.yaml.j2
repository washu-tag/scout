proxy:
  service:
    type: ClusterIP

hub:
  resources: {{ jupyter_hub_resources | to_json }}
  shutdownOnLogout: true
  services:
    prometheus:
      admin: false
      # Secret would be preferred, but not clear how to do that in this chart
      api_token: '{{ jupyter_metrics_api_token }}'
    mcp:
      admin: false
      api_token: '{{ jupyter_mcp_api_token }}'
  loadRoles:
    metrics:
      description: 'Read JupyterHub metrics'
      scopes: [read:metrics]
      services: [prometheus]
    mcp:
      description: 'Allow MCP sidecars to access single-user server APIs'
      scopes: [access:servers]
      services: [mcp]
  db:
    pvc:
      storageClassName: '{{ use_custom_storage_classes | ternary(jupyter_hub_storage_class, omit) }}'
  config:
    Authenticator:
      auto_login: true
    JupyterHub:
      subdomain_host: 'jupyter.{{ server_hostname }}'
      authenticator_class: 'oauthenticator.generic.GenericOAuthenticator'
    GenericOAuthenticator:
      login_service: 'Keycloak'
      client_id: '{{ keycloak_jupyterhub_client_id }}'
      client_secret: '{{ keycloak_jupyterhub_client_secret }}'
      oauth_callback_url: 'https://jupyter.{{ server_hostname }}/hub/oauth_callback'
      authorize_url: '{{ keycloak_oidc_auth_url }}'
      token_url: '{{ keycloak_oidc_token_url }}'
      userdata_url: '{{ keycloak_oidc_userinfo_url }}'
      logout_redirect_url: '{{ oauth2_proxy_signout_url }}'
      userdata_params:
        state: 'state'
      username_claim: 'preferred_username'
      scope:
        - openid
        - microprofile-jwt
      enable_auth_state: true
      auth_state_groups_key: 'oauth_user.groups'
      manage_groups: true
      allowed_groups:
        - 'jupyterhub-user'
      admin_groups:
        - 'jupyterhub-admin'
    Spawner:
      http_timeout: 120

ingress:
  enabled: true
  ingressClassName: traefik
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: >
      kube-system-oauth2-proxy-error@kubernetescrd,
      kube-system-oauth2-proxy-auth@kubernetescrd
  hosts:
    - 'jupyter.{{ server_hostname }}'
    - '*.jupyter.{{ server_hostname }}'

prePuller:
  hook:
    enabled: true
  continuous:
    enabled: {{ jupyter_prepuller_continuous | default(false) }}

singleuser:
  image:
    name: '{{ jupyter_singleuser_image_name }}'
    tag: '{{ jupyter_singleuser_image_tag }}'
    pullPolicy: IfNotPresent
  profileList: {{ jupyter_profiles | to_json }}
  cmd: null
  initContainers:
    - name: spark-config-setup
      image: '{{ jupyter_singleuser_image_name }}:{{ jupyter_singleuser_image_tag }}'
      command: ["/bin/sh", "-c"]
      args:
        - |
          # Copy original ConfigMap to writable location
          # The initContainer does not have the necessary env vars to make the needed updates,
          # so we just copy the original file here and make updates in the postStart hook
          cp /mnt/spark-config-original/spark-defaults.conf /mnt/spark-config/spark-defaults.conf
      volumeMounts:
        - name: spark-defaults
          mountPath: /mnt/spark-config-original
          readOnly: true
        - name: spark-config-writable
          mountPath: /mnt/spark-config
  lifecycleHooks:
    postStart:
      exec:
        command:
          - '/bin/sh'
          - '-c'
          - |
            # Append Spark memory settings based on profile environment variables
            if [ -n "$SPARK_DRIVER_MEMORY" ]; then
              echo "spark.driver.memory $SPARK_DRIVER_MEMORY" >> /usr/local/spark/conf/spark-defaults.conf
            fi
            if [ -n "$SPARK_EXECUTOR_MEMORY" ]; then
              echo "spark.executor.memory $SPARK_EXECUTOR_MEMORY" >> /usr/local/spark/conf/spark-defaults.conf
            fi

            # Copy Scout quickstart samples on first login
            FLAG_FILE=/home/$NB_USER/.scout_quickstart
            if [ ! -f $FLAG_FILE ]; then
              mkdir -p /home/$NB_USER/models
              mkdir -p /home/$NB_USER/Scout &&
              cp -r /opt/scout/samples/* /home/$NB_USER/Scout/ &&
              chown -R $NB_USER:$NB_GID /home/$NB_USER/Scout &&
              touch $FLAG_FILE
            fi
  extraEnv:
    JUPYTERHUB_ALLOW_TOKEN_IN_URL: '1'
    # HuggingFace model cache directory - models downloaded from HF Hub are stored here
    # This directory is created in postStart hook and persists with user's home directory
    HF_HOME: '/home/jovyan/models'
    # Trino connection information for read-only access from notebooks
    # Users can connect with: trino.dbapi.connect(host=os.getenv('TRINO_HOST'), ...)
    TRINO_HOST: 'trino.{{ trino_namespace }}'
    TRINO_PORT: '8080'
    TRINO_SCHEME: 'http'
    TRINO_USER: '{{ trino_readonly_user }}'
    TRINO_PASSWORD: '{{ trino_readonly_password }}'
    TRINO_CATALOG: 'delta'
    TRINO_SCHEMA: 'default'
  extraFiles:
    jupyter-server-config:
      mountPath: /etc/jupyter/jupyter_server_config.py
      stringData: |
        # Set CSP headers to enable Voila plugin rendering in JupyterLab iframe
        c.ServerApp.tornado_settings = {
            'headers': {
                'Content-Security-Policy': "frame-ancestors 'self'"
            }
        }

  storage:
    type: static
    static:
      pvcName: '{{ jupyter_singleuser_pvc }}'
      subPath: '{username}'
    capacity: 10Gi
    extraVolumes:
      spark-defaults:
        name: spark-defaults
        configMap:
          name: spark-defaults
      spark-config-writable:
        name: spark-config-writable
        emptyDir: {}
    extraVolumeMounts:
      spark-defaults:
        name: spark-config-writable
        mountPath: /usr/local/spark/conf/spark-defaults.conf
        subPath: spark-defaults.conf
  extraContainers:
    - name: jupyter-mcp
      image: '{{ jupyter_mcp_image }}'
      imagePullPolicy: IfNotPresent
      ports:
        - containerPort: 4040
      command: ['/bin/sh', '-lc']
      args:
        - |
          set -euo pipefail
          SERVICE_PREFIX="/user/$HUB_USERNAME"
          if [ -n "$SERVER_NAME" ]; then
            SERVICE_PREFIX="$SERVICE_PREFIX/$SERVER_NAME"
          fi

          HUB_BASE="http://proxy-public.{{ jupyter_namespace }}.svc.cluster.local"
          BASE="$HUB_BASE$SERVICE_PREFIX"
          export BASE

          echo "[mcp/wait] Jupyter API"
          python -c "$WAIT_FOR_JUPYTER_PY"

          echo "[mcp] starting jupyter-mcp-server"
          exec jupyter-mcp-server start \
            --transport streamable-http \
            --provider jupyter \
            --port 4040 \
            --document-url "$BASE" \
            --runtime-url  "$BASE" \
            --document-id "$NB_PATH" \
            --start-new-runtime false \
            --document-token "$JUPYTER_MCP_SERVICE_TOKEN" \
            --runtime-token  "$JUPYTER_MCP_SERVICE_TOKEN"
      env:
        - name: JUPYTER_MCP_SERVICE_TOKEN
          value: '{{ jupyter_mcp_api_token }}'
        - name: HUB_USERNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['hub.jupyter.org/username']
        - name: SERVER_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.labels['hub.jupyter.org/servername']
        - name: NB_PATH
          value: "notebook.ipynb"
        - name: WAIT_FOR_JUPYTER_PY
          value: |
            import os, time, urllib.request, urllib.error
            base = os.environ["BASE"]
            nb   = os.environ.get("NB_PATH","notebook.ipynb")
            tok  = os.environ.get("JUPYTER_MCP_SERVICE_TOKEN","")
            hdrs = dict([("Authorization", "token " + tok)]) if tok else dict()
            def wait(url):
                while True:
                    try:
                        req = urllib.request.Request(url, headers=hdrs)
                        urllib.request.urlopen(req, timeout=3).read()
                        break
                    except Exception:
                        time.sleep(1)
            wait(base + "/api/kernels")

    - name: mcpo
      image: '{{ mcpo_image }}'
      imagePullPolicy: IfNotPresent
      ports:
        - containerPort: 8000
      command: ['/bin/sh', '-c']
      args:
        - |
          set -e
          # Wait for jupyter-mcp HTTP to be live
          echo "[mcpo] waiting"
          until curl -sS -o /dev/null --max-time 2 http://127.0.0.1:4040/; do
            echo "still waiting..."; sleep 2;
          done;
          echo "[mcpo] starting"
          exec uvx mcpo \
            --host 0.0.0.0 \
            --port 8000 \
            --server-type streamable-http \
            -- \
            http://127.0.0.1:4040/mcp
  networkPolicy:
    egress:
      # Allow access to MinIO for data lake storage
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ minio_tenant_namespace }}'
        ports:
          - port: 9000
      # Allow access to Hive Metastore (readonly) for metadata queries
      # Only allow access to readonly metastore, not write metastore
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ hive_namespace }}'
            podSelector:
              matchLabels:
                app.kubernetes.io/instance: '{{ hive_metastore_instance_readonly }}'
        ports:
          - port: 9083
      # Allow access to Ollama for AI model inference via MCP
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ chatbot_namespace }}'
        ports:
          - port: 11434
      # Allow access to Trino for SQL queries via MCP
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ trino_namespace }}'
        ports:
          - port: 8080
      # Allow access to JupyterHub proxy for MCP inter-notebook communication
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ jupyter_namespace }}'
        ports:
          - port: 80
