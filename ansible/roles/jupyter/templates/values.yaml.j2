proxy:
  service:
    type: ClusterIP

hub:
  resources: {{ jupyter_hub_resources | to_json }}
  shutdownOnLogout: true
  services:
    prometheus:
      admin: false
      # Secret would be preferred, but not clear how to do that in this chart
      api_token: '{{ jupyter_metrics_api_token }}'
  loadRoles:
    metrics:
      description: 'Read JupyterHub metrics'
      scopes: [read:metrics]
      services: [prometheus]
{% if jupyterhub_storage_class is defined and jupyterhub_storage_class %}
  db:
    pvc:
      storageClassName: '{{ jupyterhub_storage_class }}'
      storage: '{{ jupyter_hub_storage_size }}'
{% endif %}
  extraVolumes:
    - name: scout-logo
      configMap:
        name: scout-logo
        defaultMode: 0644
  extraVolumeMounts:
    - name: scout-logo
      mountPath: /usr/local/share/jupyterhub/static/images/scout.png
      subPath: scout.png
      readOnly: true
  extraConfig:
    branding: |
      c.JupyterHub.logo_file = '/usr/local/share/jupyterhub/static/images/scout.png'
  config:
    Authenticator:
      auto_login: true
    JupyterHub:
      subdomain_host: 'jupyter.{{ server_hostname }}'
      authenticator_class: 'oauthenticator.generic.GenericOAuthenticator'
    GenericOAuthenticator:
      login_service: 'Keycloak'
      client_id: '{{ keycloak_jupyterhub_client_id }}'
      client_secret: '{{ keycloak_jupyterhub_client_secret }}'
      oauth_callback_url: 'https://jupyter.{{ server_hostname }}/hub/oauth_callback'
      authorize_url: '{{ keycloak_oidc_auth_url }}'
      token_url: '{{ keycloak_oidc_token_url }}'
      userdata_url: '{{ keycloak_oidc_userinfo_url }}'
      logout_redirect_url: '{{ oauth2_proxy_signout_url }}'
      userdata_params:
        state: 'state'
      username_claim: 'preferred_username'
      scope:
        - openid
        - microprofile-jwt
      enable_auth_state: true
      auth_state_groups_key: 'oauth_user.groups'
      manage_groups: true
      allowed_groups:
        - 'jupyterhub-user'
      admin_groups:
        - 'jupyterhub-admin'
    Spawner:
      http_timeout: 120

ingress:
  enabled: true
  ingressClassName: traefik
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: >
      kube-system-oauth2-proxy-error@kubernetescrd,
      kube-system-oauth2-proxy-auth@kubernetescrd
  hosts:
    - 'jupyter.{{ server_hostname }}'
    - '*.jupyter.{{ server_hostname }}'

prePuller:
  hook:
    enabled: true
  continuous:
    enabled: {{ jupyter_prepuller_continuous | default(false) }}

singleuser:
  image:
    name: '{{ jupyter_singleuser_image_name }}'
    tag: '{{ jupyter_singleuser_image_tag }}'
    pullPolicy: IfNotPresent
  profileList: {{ jupyter_profiles | to_json }}
  cmd: null
  initContainers:
    - name: spark-config-setup
      image: '{{ jupyter_singleuser_image_name }}:{{ jupyter_singleuser_image_tag }}'
      command: ["/bin/sh", "-c"]
      args:
        - |
          # Copy original ConfigMap to writable location
          # The initContainer does not have the necessary env vars to make the needed updates,
          # so we just copy the original file here and make updates in the postStart hook
          cp /mnt/spark-config-original/spark-defaults.conf /mnt/spark-config/spark-defaults.conf
      volumeMounts:
        - name: spark-defaults
          mountPath: /mnt/spark-config-original
          readOnly: true
        - name: spark-config-writable
          mountPath: /mnt/spark-config
  lifecycleHooks:
    postStart:
      exec:
        command:
          - '/bin/sh'
          - '-c'
          - |
            # Append Spark memory settings based on profile environment variables
            if [ -n "$SPARK_DRIVER_MEMORY" ]; then
              echo "spark.driver.memory $SPARK_DRIVER_MEMORY" >> /usr/local/spark/conf/spark-defaults.conf
            fi
            if [ -n "$SPARK_EXECUTOR_MEMORY" ]; then
              echo "spark.executor.memory $SPARK_EXECUTOR_MEMORY" >> /usr/local/spark/conf/spark-defaults.conf
            fi

            # Copy branding files to .jupyter/custom/ (must be done as UID 1000, not by K8s mounts)
            # This avoids BeeGFS root squash creating .jupyter/ as nobody:nobody
            mkdir -p /home/$NB_USER/.jupyter/custom
            cp /opt/scout-branding/custom.css /home/$NB_USER/.jupyter/custom/
            cp /opt/scout-branding/scout.png /home/$NB_USER/.jupyter/custom/

            # Copy Scout quickstart samples on first login
            FLAG_FILE=/home/$NB_USER/.scout_quickstart
            if [ ! -f $FLAG_FILE ]; then
              mkdir -p /home/$NB_USER/models
              mkdir -p /home/$NB_USER/Scout &&
              cp -r /opt/scout/samples/* /home/$NB_USER/Scout/ &&
              chown -R $NB_USER:$NB_GID /home/$NB_USER/Scout &&
              touch $FLAG_FILE
            fi
  extraEnv:
    # HuggingFace model cache directory - models downloaded from HF Hub are stored here
    # This directory is created in postStart hook and persists with user's home directory
    HF_HOME: '/home/jovyan/models'
    # Move IPython directory to local emptyDir to avoid SQLite locking issues on BeeGFS
    IPYTHONDIR: '/home/jovyan/.local/share/jupyter/ipython'
    # Trino connection information for read-only access from notebooks
    # Users can connect with: trino.dbapi.connect(host=os.getenv('TRINO_HOST'), ...)
    TRINO_HOST: 'trino.{{ trino_namespace }}'
    TRINO_PORT: '8080'
    TRINO_SCHEME: 'http'
    TRINO_USER: '{{ trino_user }}'
    TRINO_CATALOG: 'delta'
    TRINO_SCHEMA: 'default'
  extraFiles:
    jupyter-server-config:
      mountPath: /etc/jupyter/jupyter_server_config.py
      stringData: |
        # Set CSP headers to enable Voila plugin rendering in JupyterLab iframe
        c.ServerApp.tornado_settings = {
            'headers': {
                'Content-Security-Policy': "frame-ancestors 'self'"
            }
        }

        # Enable custom CSS loading for Scout branding
        c.LabApp.custom_css = True

        # Move YStore SQLite database to local emptyDir to avoid BeeGFS locking issues
        c.SQLiteYStore.db_path = '/home/jovyan/.local/share/jupyter/ystore.db'

        # Disable IPython history SQLite to avoid BeeGFS locking issues
        c.HistoryManager.enabled = False

  storage:
    type: static
    static:
      pvcName: '{{ jupyter_singleuser_pvc }}'
      subPath: '{username}'
    capacity: 10Gi
    extraVolumes:
      spark-defaults:
        name: spark-defaults
        configMap:
          name: spark-defaults
      spark-config-writable:
        name: spark-config-writable
        emptyDir: {}
      jupyterlab-custom-css:
        name: jupyterlab-custom-css
        configMap:
          name: jupyterlab-custom-css
      jupyterlab-scout-logo:
        name: jupyterlab-scout-logo
        configMap:
          name: scout-logo
      # Local emptyDir for SQLite databases - avoids file locking issues on BeeGFS
      jupyter-local-data:
        name: jupyter-local-data
        emptyDir: {}
    extraVolumeMounts:
      # Mount local storage for SQLite (FileIdManager, jupyter_ai indices, runtime)
      # This is ephemeral but avoids SQLite locking issues on network filesystems
      jupyter-local-data:
        name: jupyter-local-data
        mountPath: /home/jovyan/.local/share/jupyter
      spark-defaults:
        name: spark-config-writable
        mountPath: /usr/local/spark/conf/spark-defaults.conf
        subPath: spark-defaults.conf
      # Mount branding files to /opt (not /home/jovyan) to avoid Kubernetes creating
      # .jupyter/ directory as root (which gets squashed to nobody on BeeGFS)
      jupyterlab-custom-css:
        name: jupyterlab-custom-css
        mountPath: /opt/scout-branding/custom.css
        subPath: custom.css
      jupyterlab-scout-logo:
        name: jupyterlab-scout-logo
        mountPath: /opt/scout-branding/scout.png
        subPath: scout.png
  networkPolicy:
    egress:
      # Allow access to MinIO for data lake storage
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ minio_tenant_namespace }}'
        ports:
          - port: 9000
      # Allow access to Hive Metastore (readonly) for metadata queries
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ hive_namespace }}'
            podSelector:
              matchLabels:
                app.kubernetes.io/instance: '{{ hive_metastore_instance_readonly }}'
        ports:
          - port: 9083
      # Allow access to Ollama for AI model inference
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ chatbot_namespace }}'
        ports:
          - port: 11434
      # Allow access to Trino for SQL queries
      - to:
          - namespaceSelector:
              matchLabels:
                kubernetes.io/metadata.name: '{{ trino_namespace }}'
        ports:
          - port: 8080
