---
# Global settings that apply to all hosts
all:
  vars:
    # SSH connection and privilege escalation
    # There are many other params you could set here to customize your situation.
    # See https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html#connecting-to-hosts-behavioral-inventory-parameters
    ansible_user: '<ssh username>'
    ansible_become: true
    ansible_become_method: sudo
    ansible_become_user: root
    ansible_become_password: $(echo $ANSIBLE_BECOME_PASSWORD | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)

    # K3s kubeconfig path, modify if not using K3s
    # kubeconfig_yaml: /etc/rancher/k3s/k3s.yaml

    # Air-gapped deployment configuration
    # Set to true to enable air-gapped deployment mode for Kubernetes clusters
    # without internet access on worker nodes.
    #
    # When enabled (air_gapped: true):
    #   - Helm charts deployed from localhost using remote kubeconfig
    #   - Full Helm functionality retained (rollback, history, hooks)
    #   - Container images pulled through Harbor registry proxy on staging node
    #   - Requires: staging group defined, Harbor deployed, registry mirrors configured
    #   - Requires: k8s API accessible from localhost (port 6443)
    #
    # When disabled (air_gapped: false, default):
    #   - Standard Helm deployment using kubernetes.core.helm module
    #   - Charts pulled directly from public repositories
    #   - Container images pulled directly from public registries
    #   - Works with existing Scout deployments (backward compatible)
    # air_gapped: false  # Default is false; set to true here to override

# The staging group contains the staging node that acts as a data diode
# for air-gapped deployments.
#
# Role: The staging node hosts a Harbor registry that acts as a pull-through
# proxy for container images from public registries (Docker Hub, GitHub Container
# Registry, Quay.io, etc.). It runs a separate k3s cluster from the main cluster.
#
# Requirements for air-gapped deployments (air_gapped: true):
#   1. Staging node must have internet access
#   2. Harbor must be deployed on staging node (playbooks/harbor.yaml)
#   3. K3s cluster nodes must have registry mirrors configured to use Harbor
#   4. Kubeconfig for air-gapped cluster must be accessible from localhost
#   5. K8s API port (6443) must be accessible from localhost
#
# Enable air-gapped functionality by setting `air_gapped: true`
# in all vars above.
staging:
  hosts:
    FQDN-staging.edu:
      ansible_host: staging # Host name to use for ssh connection
      ansible_python_interpreter: /usr/bin/python3
  vars:
    # Staging-specific variables (only needed if use_staging_node: true)
    # The staging node runs a separate k3s cluster from the main cluster
    staging_k3s_token: $(openssl rand -hex 16 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    harbor_admin_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    harbor_storage_size: 100Gi
    harbor_dir: /scout/persistence/harbor

# The server group contains the control plane nodes for the k3s cluster.
server:
  hosts:
    FQDN-leader.edu:
      ansible_connection: local # set if running ansible on this node. if connecting via ssh omit.
      ansible_host: leader # Host name to use for ssh connection; omit if `ansible_connection: local` OR ssh can connect using FQDN
      ansible_python_interpreter: /usr/bin/python3
      k3s_control_node: true
      external_url: alt.fqdn.edu # omit to use FQDN-leader.edu
# The workers group contains the worker nodes for the k3s cluster.
workers:
  hosts:
    FQDN-worker-1.edu:
      ansible_connection: local # set if running ansible on this node. if connecting via ssh omit.
      ansible_host: worker-1 # Host name to use for ssh connection; omit if `ansible_connection: local` OR ssh can connect using FQDN
      ansible_python_interpreter: /usr/bin/python3
gpu_workers:
  hosts:
    FQDN-gpu-1.edu:
      ansible_connection: local # set if running ansible on this node. if connecting via ssh omit.
      ansible_host: gpu-1 # Host name to use for ssh connection; omit if `ansible_connection: local` OR ssh can connect using FQDN
      ansible_python_interpreter: /usr/bin/python3
# The agents group contains nodes that run only the k3s agent.
# In this example it only includes the workers and gpu_workers groups.
agents:
  children:
    workers:
    gpu_workers:
# Include in this group any nodes on which MinIO should run.
# Note that if this group contains more than one node, the value of
#  `minio_volumes_per_server` must be greater than 1 or minio will fail to start.
minio_hosts:
  children:
    server:
    workers:
# This group contains all nodes in the k3s cluster.
k3s_cluster:
  children:
    server:
    agents:
  vars:
    # K3s config
    k3s_token: $(openssl rand -hex 16 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    kubeconfig_group: '<name of the linux group that should be able to run kubectl>'

    # Traefik config
    tls_cert_path: ''
    tls_key_path: ''

    # Minio config, ensure passwords meet complexity requirements (must be >=8 characters)
    minio_storage_size: 750Gi
    minio_volumes_per_server: 2
    s3_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    s3_lake_reader_secret: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    s3_lake_writer_secret: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    s3_loki_writer_secret: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)

    # Monitor config
    prometheus_storage_size: 100Gi
    loki_storage_size: 100Gi
    grafana_storage_size: 50Gi
    grafana_smtp_host: '' # Include the port in the value
    grafana_smtp_user: $(echo $SMTP_USER | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    grafana_smtp_password: $(echo $SMTP_PASSWORD | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    grafana_smtp_from_address: ''
    slack_token: $(echo $SLACK_TOKEN | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    slack_channel_id: $(echo $SLACK_CHANNEL | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)

    # Jupyter config
    github_client_id: $(echo $CLIENT_ID | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    github_client_secret: $(echo $CLIENT_SECRET | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    github_organization: org
    jupyter_dummy_password: $(echo $JH_DUMMY_PASSWORD | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    jupyter_metrics_api_token: $(openssl rand -hex 32  | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    jupyter_hub_storage_size: 15Gi
    jupyter_singleuser_storage_size: 250Gi
    jupyter_singleuser_extra_resource:
      guarantees:
        nvidia.com/gpu: '1'
      limits:
        nvidia.com/gpu: '1'

    # Postgres config
    # Note: Most postgres configuration is now in roles/postgres/defaults/main.yaml
    # Only secrets and production-specific overrides are needed here
    postgres_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    hive_postgres_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    superset_postgres_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    postgres_superuser_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)

    # Production resource overrides (defaults in role are for dev)
    postgres_resources:
      requests:
        cpu: 4
        memory: 64Gi
      limits:
        cpu: 6
        memory: 96Gi
    postgres_storage_size: 100Gi
    postgres_parameters: # Production tuning (defaults in role are for dev)
      max_connections: '100'
      shared_buffers: '16GB'
      effective_cache_size: '48GB'
      maintenance_work_mem: '2GB'
      checkpoint_completion_target: '0.9'
      wal_buffers: '16MB'
      default_statistics_target: '500'
      random_page_cost: '4'
      effective_io_concurrency: '1'
      work_mem: '2GB'
      huge_pages: 'try'
      min_wal_size: '4GB'
      max_wal_size: '16GB'
      max_worker_processes: '4'
      max_parallel_workers_per_gather: '2'
      max_parallel_workers: '4'
      max_parallel_maintenance_workers: '2'

    # Superset config
    superset_secret: $(openssl rand -base64 42 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)

    # Trino config
    # Note: Most configuration is now in roles/trino/defaults/main.yaml
    # Only production-specific resource overrides are needed here
    trino_worker_memory_gb: 16 # Total pod memory for workers
    trino_coordinator_memory_gb: 8 # Total pod memory for coordinator

    # Orchestrator config
    cassandra_storage_size: 300Gi
    elasticsearch_storage_size: 100Gi

    # Namespaces
    # Default namespaces are defined in roles/scout_common/defaults/main.yaml
    # Override them here if you need custom namespace names
    # Example: prometheus_namespace: my-custom-prometheus-namespace

    # Local paths
    base_dir: /var/lib/rancher/k3s/storage # Path to directory for container images and sandbox data
    scout_repo_dir: /scout/data/scout
    minio_dir: /scout/data/minio
    cassandra_dir: /scout/persistence/cassandra
    elasticsearch_dir: /scout/persistence/elasticsearch
    postgres_dir: /scout/persistence/postgres
    prometheus_dir: /scout/monitoring/prometheus
    loki_dir: /scout/monitoring/loki
    grafana_dir: /scout/monitoring/grafana
    jupyter_dir: /scout/data/jupyter
    extractor_data_dir: /ceph/input/data

    # Parameters for optional PACS instances, only intended for use in development
    orthanc_dir: /scout/data/orthanc
    dcm4chee_dir: /scout/data/dcm4chee

    # All other configuration defaults are in roles/scout_common/defaults/main.yaml
    # Override them here if needed for this environment
