---
# Global settings that apply to all hosts
all:
  vars:
    # SSH connection and privilege escalation
    # There are many other params you could set here to customize your situation.
    # See https://docs.ansible.com/ansible/latest/inventory_guide/intro_inventory.html#connecting-to-hosts-behavioral-inventory-parameters
    ansible_user: '<ssh username>'
    ansible_become_method: sudo
    ansible_become_user: root
    ansible_become_password: $(echo $ANSIBLE_BECOME_PASSWORD | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)

    # Do *not* set this to true for all, or ansible will act as root on localhost and make things difficult
    # Instead set ansible_become: true in the staging and k3s_cluster groups
    # ansible_become: true

    # K3s kubeconfig path, modify if not using K3s
    # kubeconfig_yaml: /etc/rancher/k3s/k3s.yaml

    # Air-gapped deployment configuration
    # Set to true to enable air-gapped deployment mode for Kubernetes clusters
    # without internet access on worker nodes.
    #
    # When enabled (air_gapped: true):
    #   - Helm charts deployed from localhost using remote kubeconfig
    #   - Full Helm functionality retained (rollback, history, hooks)
    #   - Container images pulled through Harbor registry proxy on staging node
    #   - Requires: staging group defined, Harbor deployed, registry mirrors configured
    #   - Requires: k8s API accessible from localhost (port 6443)
    #
    # When disabled (air_gapped: false, default):
    #   - Standard Helm deployment using kubernetes.core.helm module
    #   - Charts pulled directly from public repositories
    #   - Container images pulled directly from public registries
    #   - Works with existing Scout deployments (backward compatible)
    # air_gapped: false  # Default is false; set to true here to override

# The staging group contains the staging node that acts as a data diode
# for air-gapped deployments.
#
# Role: The staging node hosts a Harbor registry that acts as a pull-through
# proxy for container images from public registries (Docker Hub, GitHub Container
# Registry, Quay.io, etc.). It runs a separate k3s cluster from the main cluster.
#
# Requirements for air-gapped deployments (air_gapped: true):
#   1. Staging node must have internet access
#   2. Harbor must be deployed on staging node (playbooks/harbor.yaml)
#   3. K3s cluster nodes must have registry mirrors configured to use Harbor
#   4. Kubeconfig for air-gapped cluster must be accessible from localhost
#   5. K8s API port (6443) must be accessible from localhost
#
# Enable air-gapped functionality by setting `air_gapped: true`
# in all vars above.
staging:
  hosts:
    FQDN-staging.edu:
      ansible_host: staging # Host name to use for ssh connection
      ansible_python_interpreter: /usr/bin/python3
  vars:
    ansible_become: true

    # Staging-specific variables (only needed if use_staging_node: true)
    # The staging node runs a separate k3s cluster from the main cluster
    staging_k3s_token: $(openssl rand -hex 16 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    harbor_admin_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    harbor_storage_size: 100Gi
    harbor_dir: /scout/persistence/harbor

# The server group contains the control plane nodes for the k3s cluster.
server:
  hosts:
    FQDN-leader.edu:
      ansible_connection: local # set if running ansible on this node. if connecting via ssh omit.
      ansible_host: leader # Host name to use for ssh connection; omit if `ansible_connection: local` OR ssh can connect using FQDN
      ansible_python_interpreter: /usr/bin/python3
      k3s_control_node: true
      external_url: alt.fqdn.edu # omit to use FQDN-leader.edu
# The workers group contains the worker nodes for the k3s cluster.
workers:
  hosts:
    FQDN-worker-1.edu:
      ansible_connection: local # set if running ansible on this node. if connecting via ssh omit.
      ansible_host: worker-1 # Host name to use for ssh connection; omit if `ansible_connection: local` OR ssh can connect using FQDN
      ansible_python_interpreter: /usr/bin/python3
gpu_workers:
  hosts:
    FQDN-gpu-1.edu:
      ansible_connection: local # set if running ansible on this node. if connecting via ssh omit.
      ansible_host: gpu-1 # Host name to use for ssh connection; omit if `ansible_connection: local` OR ssh can connect using FQDN
      ansible_python_interpreter: /usr/bin/python3
# The agents group contains nodes that run only the k3s agent.
# In this example it only includes the workers and gpu_workers groups.
agents:
  children:
    workers:
    gpu_workers:
# Include in this group any nodes on which MinIO should run.
# Note that if this group contains more than one node, the value of
#  `minio_volumes_per_server` must be greater than 1 or minio will fail to start.
minio_hosts:
  children:
    server:
    workers:
# This group contains all nodes in the k3s cluster.
k3s_cluster:
  children:
    server:
    agents:
  vars:
    # Note: Most configuration is now in roles/<role>/defaults/main.yaml
    # or roles/scout_common/defaults/main.yaml (for variables used across roles)
    # Only secrets and production-specific resource overrides are required here,
    # though you can override any role variables you desire.
    #
    # To override versions in group_vars/all/versions.yaml, you need to pass the
    # variable to ansible with -e.
    #
    # Best practice for upgrades: Test version upgrades in your staging
    # environment before applying them to production. For example:
    #   ansible-playbook -e "k3s_version=v1.35.0+k3s1" playbooks/k3s.yaml

    # Set this to execute ansible commands as root
    ansible_become: true

    # Namespaces
    # Default namespaces are defined in roles/scout_common/defaults/main.yaml
    # Override them here if you need custom namespace names
    # Example: prometheus_namespace: my-custom-prometheus-namespace

    #############################################################################
    ### On-prem storage                                                       ###
    #############################################################################

    # Storage disk space
    postgres_storage_size: 100Gi
    cassandra_storage_size: 300Gi
    elasticsearch_storage_size: 100Gi
    jupyter_hub_storage_size: 15Gi
    jupyter_singleuser_storage_size: 250Gi
    prometheus_storage_size: 100Gi
    loki_storage_size: 100Gi
    grafana_storage_size: 50Gi
    minio_storage_size: 750Gi

    # Local paths
    base_dir: /var/lib/rancher/k3s/storage # Path to directory for container images and sandbox data
    scout_repo_dir: /scout/data/scout
    minio_dir: /scout/data/minio
    cassandra_dir: /scout/persistence/cassandra
    elasticsearch_dir: /scout/persistence/elasticsearch
    postgres_dir: /scout/persistence/postgres
    prometheus_dir: /scout/monitoring/prometheus
    loki_dir: /scout/monitoring/loki
    grafana_dir: /scout/monitoring/grafana
    jupyter_dir: /scout/data/jupyter
    extractor_data_dir: /ceph/input/data
    # Parameters for optional PACS instances, only intended for use in development
    orthanc_dir: /scout/data/orthanc
    dcm4chee_dir: /scout/data/dcm4chee

    #############################################################################
    ### Service-specific secrets and resources                                ###
    #############################################################################

    #---------------------------------------------------------------------------
    # K3s
    #---------------------------------------------------------------------------
    k3s_token: $(openssl rand -hex 16 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    kubeconfig_group: '<name of the linux group that should be able to run kubectl>'

    #---------------------------------------------------------------------------
    # Traefik
    #---------------------------------------------------------------------------
    tls_cert_path: ''
    tls_key_path: ''

    #---------------------------------------------------------------------------
    # PostgreSQL
    #---------------------------------------------------------------------------
    postgres_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    postgres_superuser_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    # Production resource overrides (defaults in role are for dev)
    postgres_resources:
      requests:
        cpu: 4
        memory: 64Gi
      limits:
        cpu: 6
        memory: 96Gi
    postgres_parameters: # Production tuning (defaults in role are for dev)
      max_connections: '100'
      shared_buffers: '16GB'
      effective_cache_size: '48GB'
      maintenance_work_mem: '2GB'
      checkpoint_completion_target: '0.9'
      wal_buffers: '16MB'
      default_statistics_target: '500'
      random_page_cost: '4'
      effective_io_concurrency: '1'
      work_mem: '2GB'
      huge_pages: 'try'
      min_wal_size: '4GB'
      max_wal_size: '16GB'
      max_worker_processes: '4'
      max_parallel_workers_per_gather: '2'
      max_parallel_workers: '4'
      max_parallel_maintenance_workers: '2'

    #---------------------------------------------------------------------------
    # Cassandra
    #---------------------------------------------------------------------------
    cassandra_init_heap: 6G
    cassandra_max_heap: 12G
    # Memory computed from max_heap (requests = 1x = 12Gi, limits = 2x = 24Gi)
    # Override CPU if needed (defaults: 250m request, 2 limit):
    cassandra_cpu_request: 2
    cassandra_cpu_limit: 4

    #---------------------------------------------------------------------------
    # Elasticsearch
    #---------------------------------------------------------------------------
    elasticsearch_max_heap: 3G
    # Memory computed from heap_size (requests = 2x = 6Gi, limits = 4x = 12Gi)
    # Note: Heap should be ≤ 50% of pod memory, and ≤ 26GB for compressed OOPs
    # Override CPU if needed (defaults: 250m request, 2 limit):
    elasticsearch_cpu_request: 1
    elasticsearch_cpu_limit: 3

    #---------------------------------------------------------------------------
    # MinIO (Object Storage)
    #---------------------------------------------------------------------------
    # Ensure passwords meet complexity requirements (must be >=8 characters)
    minio_volumes_per_server: 2 # See comment with `minio_hosts`
    s3_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    s3_lake_reader_secret: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    s3_lake_writer_secret: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    minio_resources:
      requests:
        cpu: 2
        memory: 8Gi
      limits:
        cpu: 4
        memory: 8Gi

    #---------------------------------------------------------------------------
    # Prometheus
    #---------------------------------------------------------------------------
    prometheus_resources:
      requests:
        cpu: 2
        memory: 8Gi
      limits:
        cpu: 4
        memory: 8Gi

    #---------------------------------------------------------------------------
    # Loki
    #---------------------------------------------------------------------------
    s3_loki_writer_secret: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    loki_resources:
      requests:
        cpu: 2
        memory: 8Gi
      limits:
        cpu: 4
        memory: 8Gi

    #---------------------------------------------------------------------------
    # Grafana
    #---------------------------------------------------------------------------
    grafana_alert_contact_point: slack # email or slack
    # Slack configuration
    slack_token: $(echo $SLACK_TOKEN | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    slack_channel_id: $(echo $SLACK_CHANNEL | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    # SMTP configuration
    grafana_smtp_host: '' # Include the port in the value
    grafana_smtp_user: $(echo $SMTP_USER | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    grafana_smtp_password: $(echo $SMTP_PASSWORD | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    grafana_smtp_from_address: ''
    grafana_smtp_from_name: 'Scout'
    grafana_smtp_skip_verify: false
    grafana_email_recipients: ['']
    # Resources
    grafana_resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 2
        memory: 4Gi

    #---------------------------------------------------------------------------
    # Jupyter
    #---------------------------------------------------------------------------
    # Authentication
    jupyter_metrics_api_token: $(openssl rand -hex 32  | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    jupyter_auth_class: dummy # Options: dummy or github
    # GitHub auth
    github_client_id: $(echo $CLIENT_ID | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    github_client_secret: $(echo $CLIENT_SECRET | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    github_organization: org
    # Dummy auth
    jupyter_dummy_password: $(echo $JH_DUMMY_PASSWORD | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    # Resources
    jupyter_spark_memory: 8G
    # Memory computed from spark_memory (guarantee/requests = 1x = 8G, limit = 2x = 16G)
    # Override CPU if needed (defaults: 0.25 request, 1 limit)
    # Note: JupyterHub Helm chart 4.3.x requires numeric values (not strings like "250m")
    # Use fractional cores (0.25 = 250 millicores) or whole numbers
    jupyter_singleuser_cpu_request: 2
    jupyter_singleuser_cpu_limit: 8
    # Override memory for non-Spark workloads (pandas, numpy, data loading, etc.)
    # By default, memory is computed from jupyter_spark_memory
    # Set these to give users more RAM than Spark alone needs:
    # jupyter_singleuser_memory_request: 16G
    # jupyter_singleuser_memory_limit: 32G
    jupyter_hub_resources:
      requests:
        cpu: 500m
        memory: 1G
      limits:
        cpu: 2
        memory: 2G
    jupyter_singleuser_extra_resource:
      guarantees:
        nvidia.com/gpu: '1'
      limits:
        nvidia.com/gpu: '1'

    #---------------------------------------------------------------------------
    # Temporal
    #---------------------------------------------------------------------------
    temporal_resources:
      requests:
        cpu: 1
        memory: 4Gi
      limits:
        cpu: 2
        memory: 8Gi

    #---------------------------------------------------------------------------
    # Superset
    #---------------------------------------------------------------------------
    superset_postgres_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    superset_secret: $(openssl rand -base64 42 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    superset_resources:
      requests:
        cpu: 1
        memory: 4Gi
      limits:
        cpu: 2
        memory: 8Gi

    #---------------------------------------------------------------------------
    # Hive Metastore
    #---------------------------------------------------------------------------
    hive_postgres_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    hive_readonly_postgres_password: $(openssl rand -hex 32 | ansible-vault encrypt_string --vault-password-file vault/pwd.sh)
    hive_resources:
      requests:
        cpu: 1
        memory: 4Gi
      limits:
        cpu: 2
        memory: 4Gi

    #---------------------------------------------------------------------------
    # Trino
    #---------------------------------------------------------------------------
    trino_worker_count: 2 # Number of worker replicas
    trino_worker_max_heap: 12G
    trino_coordinator_max_heap: 6G
    # Override CPU if needed (defaults: 250m request, 2/1 limit for worker/coordinator):
    trino_worker_cpu_request: 2
    trino_worker_cpu_limit: 6
    trino_coordinator_cpu_request: 1
    trino_coordinator_cpu_limit: 3
    # Override query memory allocation if needed (default: 0.3 = 30% of heap per Trino docs)
    # trino_per_node_query_memory_fraction: 0.3

    #---------------------------------------------------------------------------
    # Extractor
    #---------------------------------------------------------------------------
    hl7log_extractor_resources:
      requests:
        cpu: 2
        memory: 4Gi
      limits:
        cpu: 4
        memory: 8Gi
    hl7_transformer_spark_memory: 16G
    # Memory computed from spark_memory (requests = 1x = 16Gi, limits = 2x = 32Gi)
    # Override CPU if needed (defaults: 250m request, 4 limit):
    hl7_transformer_cpu_request: 2
    hl7_transformer_cpu_limit: 4
