spring:
  application:
    name: temporal-java
  temporal:
    connection:
      target: 'temporal-frontend.temporal:7233'
    namespace: default
    start-workers: true
    workers-auto-discovery:
      packages:
        - edu.washu.tag.temporal.activity
        - edu.washu.tag.temporal.workflow
s3:
  endpoint: 'http://minio.minio:9000'
  region: us-east-1
scout:
  max-children: 150 # This is to limit the memory load on Spark in the python worker, set based on an 8GB memory limit and the data
                    # that is generated from 1.3-1.4MB log files. Based on manual testing, it seems like the delta lake generated from
                    # 250MB of input logs can be written without encountering an OOM in Spark, but performance is a little better
                    # with 200MB of input logs.
