spring:
  application:
    name: temporal-java
  temporal:
    connection:
      target: 'temporal-frontend.temporal:7233'
    namespace: default
    start-workers: true
    workers-auto-discovery:
      packages:
        - edu.washu.tag.temporal.activity
        - edu.washu.tag.temporal.workflow
s3:
  endpoint: 'http://minio.minio:9000'
  region: us-east-1
scout:
  max-children:
    150 # This is to limit the memory load on Spark in the python worker, set based on an 8GB memory limit and the data
    # that is generated from 1.3-1.4MB log files. Based on manual testing, it seems like the delta lake generated from
    # 250MB of input logs can be written without encountering an OOM in Spark, but performance is a little better with
    # 200MB of input logs.
