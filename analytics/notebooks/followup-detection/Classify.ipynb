{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": "# Follow-up Detection Production Pipeline\n\nThis notebook implements an efficient follow-up detection algorithm for radiology reports using GPT-OSS:20b.\n\n**Architecture:**\n- Parallel classification with GPT-OSS:20b using ThreadPoolExecutor (16 workers)\n- Results written back to `latest_reports` with confidence scoring\n- Errors logged to `followup_errors` table for debugging\n\n**Performance:**\n- Throughput: ~2.0 reports/second with 16 workers\n- Estimated Runtime: ~5.8 days for 1M reports\n\n**Columns Added to `latest_reports`:**\n- `followup_detected` (boolean): True if follow-up recommended\n- `followup_confidence` (string): \"high\" or \"low\"\n- `followup_snippet` (string): Exact text indicating follow-up\n- `followup_finding` (string): Finding type from LLM classification (e.g., \"Pulmonary nodule\")\n- `followup_processed_at` (timestamp): Processing timestamp\n\n**Error Handling:**\n- Processing errors are logged to `followup_errors` table with report ID and error message\n- Useful for debugging LLM issues, JSON parsing failures, or timeout errors"
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  Configuration:\n",
      "  Ollama URL: http://ollama-gpu1.ollama-dual:11434\n",
      "  Model: gpt-oss:20b\n",
      "  Total workers: 16\n",
      "  Batch size: 1,000\n",
      "  Priority filter: Keyword matches first\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://ollama-gpu1.ollama-dual:11434\")\n",
    "MODEL = \"gpt-oss:20b\"\n",
    "S3_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\", \"lake-writer\")\n",
    "S3_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\", \"f8bae22e21dd400ba8b8b245b5b7c5ad62e4667ae201e411006caa55225ec2b4\")\n",
    "\n",
    "if not S3_SECRET_KEY:\n",
    "    from getpass import getpass\n",
    "    S3_SECRET_KEY = getpass(\"Enter S3 secret key: \")\n",
    "\n",
    "# Threading configuration\n",
    "TOTAL_WORKERS = 16  # Match OLLAMA_NUM_PARALLEL for optimal throughput\n",
    "\n",
    "# Processing configuration\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# Priority filtering - process reports with follow-up keywords first\n",
    "FOLLOWUP_KEYWORDS = [\n",
    "    r\"follow.?up\", r\"repeat\", r\"recommend\", r\"suggest\", r\"interval\",\n",
    "    r\"re-?evaluate\", r\"re-?assess\", r\"correlat\", r\"clinical\", r\"short.?term\",\n",
    "    r\"further\", r\"additional\", r\"consider\", r\"refer\", r\"return\",\n",
    "    r\"continue\", r\"monitor\", r\"surveillance\", r\"re-?exam\", r\"comparison\"\n",
    "]\n",
    "PRIORITY_FILTER = f\"lower(report_text) RLIKE '{('|'.join(FOLLOWUP_KEYWORDS))}'\"\n",
    "\n",
    "# Table names\n",
    "SRC_TABLE = \"default.reports\"\n",
    "DEST_TABLE = \"default.latest_reports\"\n",
    "ERROR_TABLE = \"default.followup_errors\"\n",
    "\n",
    "print(f\"âš™ï¸  Configuration:\")\n",
    "print(f\"  Ollama URL: {OLLAMA_URL}\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Total workers: {TOTAL_WORKERS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE:,}\")\n",
    "print(f\"  Priority filter: Keyword matches first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Initialize Spark & Verify Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "âœ… Spark connected to Delta Lake\n"
     ]
    }
   ],
   "source": [
    "# Start Spark session with optimizations\n",
    "spark = SparkSession.builder.appName(\"followup-detection\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore.hive:9083\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"800\") \\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "    print(\"âœ… Spark connected to Delta Lake\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to Delta Lake: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Define Classification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Classification function defined\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Does this report recommend follow-up for THIS patient?\n",
    "\n",
    "Follow-up = patient-specific imaging/evaluation for findings in THIS report\n",
    "\n",
    "YES if finding-specific action:\n",
    "- \"Repeat CT in 6 months for nodule\"\n",
    "- \"Clinical correlation recommended for lesion\"\n",
    "- Template applies: Patient HAS osteoporosis + template says \"osteoporosis needs follow-up\"\n",
    "\n",
    "NO if:\n",
    "- Normal: \"unremarkable\", \"stable\", \"no acute findings\"\n",
    "- Template doesn't apply: Patient normal + template mentions other conditions\n",
    "- Generic wellness: \"all patients should take calcium\"\n",
    "- Follow-up exam completed, no future action requested\n",
    "\n",
    "Check FINDINGS/IMPRESSION to see if templates apply to this patient.\n",
    "\n",
    "Return ONLY valid JSON, nothing else:\n",
    "{{\"follow_up\": true/false, \"confidence\": \"high\"/\"low\", \"snippet\": \"text\", \"finding\": \"text\"}}\n",
    "\n",
    "If follow_up=false: snippet=\"\" and finding=\"\"\n",
    "If follow_up=true: \n",
    "  - snippet=verbatim snippet of text from report recommending follow-up\n",
    "  - finding=specific abnormality needing follow-up (e.g., \"pulmonary nodule\", \"liver lesion\")\n",
    "\n",
    "Report:\n",
    "{report_text}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "def classify_report(row):\n",
    "    \"\"\"Classify single report via Ollama API\"\"\"\n",
    "    try:\n",
    "        prompt = PROMPT_TEMPLATE.format(report_text=row.report_text)\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/api/generate\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": 0,\n",
    "                \"stream\": False,\n",
    "                \"thinking\": \"low\",\n",
    "                \"options\": {\n",
    "                    \"num_predict\": 1000,  # Prevent JSON truncation\n",
    "                    \"num_ctx\": 4096\n",
    "                }\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
    "        \n",
    "        resp_text = response.json()[\"response\"].strip()\n",
    "\n",
    "        # Check if response is empty\n",
    "        if not resp_text:\n",
    "            raise Exception(\"Empty response from model\")\n",
    "\n",
    "        # Extract JSON object, ignoring any text before or after\n",
    "        \n",
    "        # Remove code block fencing if present\n",
    "        if resp_text.startswith(\"```json\"):\n",
    "            resp_text = resp_text[7:]  # Remove ```json\n",
    "        if resp_text.startswith(\"```\"):\n",
    "            resp_text = resp_text[3:]  # Remove ```\n",
    "\n",
    "        # Remove closing fence\n",
    "        if \"```\" in resp_text:\n",
    "            resp_text = resp_text.split(\"```\")[0]\n",
    "        \n",
    "        resp_text = resp_text.strip()\n",
    "        \n",
    "        # Check again after cleanup\n",
    "        if not resp_text:\n",
    "            raise Exception(\"Empty response after cleanup\")\n",
    "        \n",
    "        # Find first JSON object (handles text before/after)\n",
    "        json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', resp_text)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "        else:\n",
    "            # Fallback: try to parse the whole thing\n",
    "            json_str = resp_text.strip()\n",
    "\n",
    "        # Final check before parsing\n",
    "        if not json_str or not json_str.strip():\n",
    "            raise Exception(f\"No JSON found in response: {resp_text}\")\n",
    "        \n",
    "        # Parse JSON\n",
    "        obj = json.loads(json_str)\n",
    "        \n",
    "        if not isinstance(obj, dict):\n",
    "            raise ValueError(\"Response is not a JSON object\")\n",
    "        \n",
    "        # Extract and normalize values\n",
    "        follow_up = bool(obj.get(\"follow_up\", False))\n",
    "        confidence = str(obj.get(\"confidence\", \"low\")).lower()\n",
    "        if confidence not in [\"high\", \"low\"]:\n",
    "            confidence = \"low\"\n",
    "        \n",
    "        return {\n",
    "            \"message_control_id\": row.message_control_id,\n",
    "            \"followup_detected\": follow_up,\n",
    "            \"followup_confidence\": confidence,\n",
    "            \"followup_snippet\": str(obj.get(\"snippet\", \"\")),\n",
    "            \"followup_finding\": str(obj.get(\"finding\", \"\")),\n",
    "            \"error\": None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"message_control_id\": row.message_control_id,\n",
    "            \"followup_detected\": None,\n",
    "            \"followup_confidence\": None,\n",
    "            \"followup_snippet\": None,\n",
    "            \"followup_finding\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "    \n",
    "print(\"âœ… Classification function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcfc6ff-eb92-411e-b83f-444d6fb53b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Processing Status:\n",
      "  Total reports: 12,875,092\n",
      "  Processed: 358,415 (2.8%)\n",
      "  Unprocessed: 12,516,677\n",
      "\n",
      "ðŸ“ˆ Follow-up Detection Results:\n",
      "+-----------------+-------------------+------+\n",
      "|followup_detected|followup_confidence| count|\n",
      "+-----------------+-------------------+------+\n",
      "|            false|               high|294719|\n",
      "|            false|                low|  3821|\n",
      "|             true|               high| 59810|\n",
      "|             true|                low|    65|\n",
      "+-----------------+-------------------+------+\n",
      "\n",
      "\n",
      "âœ… Follow-up rate: 16.7% (59,875 / 358,415)\n"
     ]
    }
   ],
   "source": [
    "# Check processing progress\n",
    "df_status = spark.table(DEST_TABLE)\n",
    "\n",
    "total = df_status.count()\n",
    "processed = df_status.filter(F.col(\"followup_processed_at\").isNotNull()).count()\n",
    "unprocessed = total - processed\n",
    "\n",
    "print(\"ðŸ“Š Processing Status:\")\n",
    "print(f\"  Total reports: {total:,}\")\n",
    "print(f\"  Processed: {processed:,} ({100*processed/total:.1f}%)\")\n",
    "print(f\"  Unprocessed: {unprocessed:,}\")\n",
    "\n",
    "# Breakdown by follow-up status (for processed reports only)\n",
    "if processed > 0:\n",
    "  followup_stats = (df_status\n",
    "                    .filter(F.col(\"followup_processed_at\").isNotNull())\n",
    "                    .groupBy(\"followup_detected\", \"followup_confidence\")\n",
    "                    .count()\n",
    "                    .orderBy(\"followup_detected\", \"followup_confidence\"))\n",
    "\n",
    "  print(\"\\nðŸ“ˆ Follow-up Detection Results:\")\n",
    "  followup_stats.show()\n",
    "\n",
    "  # Summary metrics\n",
    "  followup_count = df_status.filter(F.col(\"followup_detected\") == True).count()\n",
    "  print(f\"\\nâœ… Follow-up rate: {100*followup_count/processed:.1f}% ({followup_count:,} / {processed:,})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Process Reports with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading model into memory...\n",
      "âœ… Model loaded into memory\n",
      "\n",
      "ðŸŽ¯ Priority Filter Statistics:\n",
      "  Priority reports (with keywords): 11,215,131 (89.6%)\n",
      "  Non-priority reports: 1,301,546 (10.4%)\n",
      "\n",
      "ðŸ“Š Processing Summary:\n",
      "  Total reports: 12,516,677\n",
      "  Batch size: 1,000\n",
      "  Batches: 12517\n",
      "  Workers: 16\n",
      "  Estimated time: 108.7 hours\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe17ecedb934386a7f1255e926f5aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe1d11787734b8c87898992985fa4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Batch 1 reports:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ’¾ Converting 1000 results to Spark DataFrame...\n",
      "  ðŸ“Š Counting successes and errors...\n",
      "     âœ“ 998 successful, 2 errors\n",
      "  âš ï¸  Writing 2 errors to default.followup_errors...\n",
      "     âœ“ Errors logged\n",
      "  ðŸ”„ Merging 998 results into default.latest_reports...\n",
      "     âœ“ Merge complete\n",
      "  ðŸ“ˆ Calculating progress...\n",
      "  ðŸ“ˆ Batch 1/12517: 998 processed (0.0%), 12,515,679 remaining | 0.85 reports/sec, 2 errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e10ab3c25264a5da8dcf903ccc57537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Batch 2 reports:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc69f65f72224bf2add4f1e7bf96521f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Batch 3 reports:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ’¾ Converting 1000 results to Spark DataFrame...\n",
      "  ðŸ“Š Counting successes and errors...\n",
      "     âœ“ 995 successful, 5 errors\n",
      "  âš ï¸  Writing 5 errors to default.followup_errors...\n",
      "     âœ“ Errors logged\n",
      "  ðŸ”„ Merging 995 results into default.latest_reports...\n",
      "     âœ“ Merge complete\n",
      "  ðŸ“ˆ Calculating progress...\n",
      "  ðŸ“ˆ Batch 3/12517: 2,990 processed (0.0%), 12,513,687 remaining | 0.90 reports/sec, 5 errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c0d12d3bf24773a9d0d925cab2cab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Batch 4 reports:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ðŸ’¾ Converting 1000 results to Spark DataFrame...\n",
      "  ðŸ“Š Counting successes and errors...\n",
      "     âœ“ 995 successful, 5 errors\n",
      "  âš ï¸  Writing 5 errors to default.followup_errors...\n",
      "     âœ“ Errors logged\n",
      "  ðŸ”„ Merging 995 results into default.latest_reports...\n",
      "     âœ“ Merge complete\n",
      "  ðŸ“ˆ Calculating progress...\n",
      "  ðŸ“ˆ Batch 4/12517: 3,985 processed (0.0%), 12,512,692 remaining | 0.85 reports/sec, 5 errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a072f365c246aba30658374a681a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  Batch 5 reports:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import parallel processing module\n",
    "from parallel_processor import process_reports_in_batches\n",
    "\n",
    "# Process all reports using simple ThreadPoolExecutor approach\n",
    "# Priority filter processes keyword matches first\n",
    "stats = process_reports_in_batches(\n",
    "    spark=spark,\n",
    "    classify_func=classify_report,\n",
    "    dest_table=DEST_TABLE,\n",
    "    error_table=ERROR_TABLE,\n",
    "    ollama_url=OLLAMA_URL,\n",
    "    model=MODEL,\n",
    "    total_workers=TOTAL_WORKERS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    priority_filter=PRIORITY_FILTER  # Process keyword matches first\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Processing complete!\")\n",
    "print(f\"\\nðŸ“Š Final Statistics:\")\n",
    "print(f\"  Total processed: {stats['total_processed']:,}\")\n",
    "print(f\"  Total errors: {stats['total_errors']:,}\")\n",
    "print(f\"  Total time: {stats['elapsed_time']/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Validation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df_final = spark.table(DEST_TABLE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š FOLLOW-UP DETECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nResults by follow-up status:\")\n",
    "df_final.groupBy(\"followup_detected\").count().orderBy(\"followup_detected\").show()\n",
    "\n",
    "print(\"\\nResults by follow-up status and confidence:\")\n",
    "df_final.groupBy(\"followup_detected\", \"followup_confidence\").count().orderBy(\"followup_detected\", \"followup_confidence\").show()\n",
    "\n",
    "print(\"\\nProcessing statistics:\")\n",
    "processed = df_final.filter(F.col(\"followup_processed_at\").isNotNull()).count()\n",
    "print(f\"  Total reports: {report_count:,}\")\n",
    "print(f\"  Processed: {processed:,} ({100*processed/report_count:.1f}%)\")\n",
    "print(f\"  Unprocessed: {report_count - processed:,}\")\n",
    "\n",
    "followup_count = df_final.filter(F.col(\"followup_detected\") == True).count()\n",
    "if processed > 0:\n",
    "    print(f\"  Follow-up rate: {100*followup_count/processed:.1f}%\")\n",
    "\n",
    "followup_high_conf = df_final.filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"high\")).count()\n",
    "if followup_count > 0:\n",
    "    print(f\"  High confidence follow-ups: {followup_high_conf:,} ({100*followup_high_conf/followup_count:.1f}%)\")\n",
    "\n",
    "print(\"\\nðŸ” Sample High Confidence Follow-up Reports:\")\n",
    "(df_final\n",
    "    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"high\"))\n",
    "    .select(\"message_control_id\", \"followup_confidence\", \"followup_snippet\")\n",
    "    .limit(10)\n",
    "    .show(truncate=80)\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ” Sample Low Confidence Follow-up Reports:\")\n",
    "(df_final\n",
    "    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"low\"))\n",
    "    .select(\"message_control_id\", \"followup_confidence\", \"followup_snippet\")\n",
    "    .limit(10)\n",
    "    .show(truncate=80)\n",
    ")\n",
    "\n",
    "# Check errors\n",
    "df_errors = spark.table(ERROR_TABLE)\n",
    "error_count = df_errors.count()\n",
    "print(f\"\\nâš ï¸  Error Statistics:\")\n",
    "print(f\"  Total errors: {error_count:,}\")\n",
    "if error_count > 0:\n",
    "    print(f\"  Error rate: {100*error_count/(processed+error_count):.2f}%\")\n",
    "    print(\"\\n  Top error types:\")\n",
    "    df_errors.groupBy(\"error\").count().orderBy(F.col(\"count\").desc()).limit(5).show(truncate=80)\n",
    "\n",
    "# Export samples for validation\n",
    "print(\"\\nðŸ’¾ Exporting validation samples...\")\n",
    "output_dir = \"/home/jovyan/followup_validation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# High confidence positive samples\n",
    "high_conf_sample = (df_final\n",
    "    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"high\"))\n",
    "    .select(\"message_control_id\", \"followup_detected\", \"followup_confidence\", \"followup_snippet\", \"report_text\")\n",
    "    .limit(100)\n",
    "    .toPandas())\n",
    "high_conf_sample.to_csv(f\"{output_dir}/followup_high_confidence_sample.csv\", index=False)\n",
    "print(f\"  âœ… Saved: {output_dir}/followup_high_confidence_sample.csv\")\n",
    "\n",
    "# Low confidence positive samples\n",
    "low_conf_sample = (df_final\n",
    "    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"low\"))\n",
    "    .select(\"message_control_id\", \"followup_detected\", \"followup_confidence\", \"followup_snippet\", \"report_text\")\n",
    "    .limit(100)\n",
    "    .toPandas())\n",
    "low_conf_sample.to_csv(f\"{output_dir}/followup_low_confidence_sample.csv\", index=False)\n",
    "print(f\"  âœ… Saved: {output_dir}/followup_low_confidence_sample.csv\")\n",
    "\n",
    "# Negative samples\n",
    "negative_sample = (df_final\n",
    "    .filter(F.col(\"followup_detected\") == False)\n",
    "    .select(\"message_control_id\", \"followup_detected\", \"followup_confidence\", \"followup_snippet\", \"report_text\")\n",
    "    .limit(100)\n",
    "    .toPandas())\n",
    "negative_sample.to_csv(f\"{output_dir}/followup_negative_sample.csv\", index=False)\n",
    "print(f\"  âœ… Saved: {output_dir}/followup_negative_sample.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1fd0f1-2815-4569-b699-1f4e5467ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Dashboard Review\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“± INTERACTIVE DASHBOARD REVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from followup_review_dashboard import create_review_dashboard\n",
    "\n",
    "# Stratified sample across modalities AND classifications\n",
    "df_processed = df_final.filter(F.col(\"followup_processed_at\").isNotNull())\n",
    "\n",
    "print(\"\\nðŸ”„ Creating stratified sample by modality...\")\n",
    "# Get 10 samples per modality per classification type\n",
    "df_review = (df_processed\n",
    "   .filter(F.col(\"modality\").isNotNull())\n",
    "   .withColumn(\"category\", F.concat_ws(\"_\",\n",
    "                                       F.col(\"modality\"),\n",
    "                                       F.col(\"followup_detected\").cast(\"string\"),\n",
    "                                       F.coalesce(F.col(\"followup_confidence\"), F.lit(\"null\"))))\n",
    "   .withColumn(\"row_num\", F.row_number().over(\n",
    "       Window.partitionBy(\"category\").orderBy(F.rand())\n",
    "   ))\n",
    "   .filter(F.col(\"row_num\") <= 10)  # 10 per category\n",
    "   .select(\"message_control_id\", \"modality\", \"followup_detected\",\n",
    "           \"followup_confidence\", \"followup_snippet\", \"report_text\")\n",
    "   .toPandas())\n",
    "\n",
    "print(f\"\\nðŸ“Š Loaded {len(df_review)} reports stratified by modality\")\n",
    "print(\"\\nSample distribution:\")\n",
    "print(df_review.groupby(['modality', 'followup_detected', 'followup_confidence']).size())\n",
    "\n",
    "# Launch dashboard\n",
    "print(\"\\nðŸš€ Launching interactive dashboard...\")\n",
    "create_review_dashboard(df_review, report_col='report_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749fdabc-d1b1-4b27-a5db-1e06eee9723f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc90d1-15f5-41c5-97f7-7264a8458419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}