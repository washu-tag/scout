{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": "# Follow-up Detection Production Pipeline\n\nThis notebook implements an efficient follow-up detection algorithm for radiology reports using GPT-OSS:20b.\n\n**Architecture:**\n- Parallel classification with GPT-OSS:20b using ThreadPoolExecutor (16 workers)\n- Results written back to `reports` with confidence scoring\n\n**Performance:**\n- Throughput: ~2.0 reports/second with 16 workers\n- Estimated Runtime: ~5.8 days for 1M reports\n\n**Columns Added:**\n- `followup_detected` (boolean): True if follow-up recommended\n- `followup_confidence` (string): \"high\" or \"low\"\n- `followup_snippet` (string): Exact text indicating follow-up\n- `followup_processed_at` (timestamp): Processing timestamp"
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Cell 1: Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": "from pyspark.sql import SparkSession, functions as F, Window\nfrom pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType\nfrom datetime import datetime\nfrom tqdm.notebook import tqdm\nimport requests\nimport json\nimport os\n\n# Configuration\nOLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://ollama.chatbot:11434\")\nMODEL = \"gpt-oss:20b\"\nS3_ACCESS_KEY = os.getenv(\"S3_ACCESS_KEY\", \"lake-writer\")\nS3_SECRET_KEY = os.getenv(\"S3_SECRET_KEY\")\n\nif not S3_SECRET_KEY:\n    from getpass import getpass\n    S3_SECRET_KEY = getpass(\"Enter S3 secret key: \")\n\n# Threading configuration\nTOTAL_WORKERS = 16  # Match OLLAMA_NUM_PARALLEL for optimal throughput\n\n# Processing configuration\nBATCH_SIZE = 1000\n\n# Priority filtering - process high-value reports first\n# Reports must have BOTH:\n# 1. Follow-up keywords (likely follow-ups)\n# 2. Complex modalities (higher clinical value)\nFOLLOWUP_KEYWORDS = [\n    r\"follow.?up\", r\"repeat\", r\"recommend\", r\"suggest\", r\"interval\",\n    r\"re-?evaluate\", r\"re-?assess\", r\"correlat\", r\"clinical\", r\"short.?term\",\n    r\"further\", r\"additional\", r\"consider\", r\"refer\", r\"return\",\n    r\"continue\", r\"monitor\", r\"surveillance\", r\"re-?exam\", r\"comparison\"\n]\nPRIORITY_MODALITIES = [\"CT\", \"MR\", \"US\", \"MG\", \"PT\"]  # PT = PET in your data\n\n# Combined priority filter: (has keywords) AND (is priority modality)\nPRIORITY_FILTER = (\n    f\"(lower(report_text) RLIKE '{('|'.join(FOLLOWUP_KEYWORDS))}') AND \"\n    f\"(modality IN ({','.join(repr(m) for m in PRIORITY_MODALITIES)}))\"\n)\n\n# Table names\nSRC_TABLE = \"default.reports\"\nDEST_TABLE = \"default.reports\"\nERROR_TABLE = \"default.followup_errors\"\n\nprint(f\"âš™ï¸  Configuration:\")\nprint(f\"  Ollama URL: {OLLAMA_URL}\")\nprint(f\"  Model: {MODEL}\")\nprint(f\"  Total workers: {TOTAL_WORKERS}\")\nprint(f\"  Batch size: {BATCH_SIZE:,}\")\nprint(f\"  Priority filter: Keywords AND Modality\")\nprint(f\"    - Must have follow-up keywords\")\nprint(f\"    - Must be in: {', '.join(PRIORITY_MODALITIES)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Cell 2: Initialize Spark & Verify Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session with optimizations\n",
    "spark = SparkSession.builder.appName(\"followup-detection\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive-metastore.hive:9083\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"800\") \\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "    print(\"âœ… Spark connected to Delta Lake\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to Delta Lake: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Cell 5: Define Classification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Does this report recommend follow-up for THIS patient?\n",
    "\n",
    "Follow-up = patient-specific imaging/evaluation based on findings\n",
    "\n",
    "YES if:\n",
    "- Specific directive: \"Repeat CT in 6 months for nodule\"\n",
    "- Referral for findings: \"Clinical correlation recommended\"\n",
    "- Template applies: Patient has osteoporosis + template says \"osteoporosis patients need follow-up\"\n",
    "\n",
    "NO if:\n",
    "- Normal findings: \"normal\", \"unremarkable\", \"stable\"\n",
    "- Template doesn't apply: Patient normal + template says \"osteoporosis patients need follow-up\"\n",
    "- Generic advice: \"all patients should take calcium\"\n",
    "- Report describes a follow-up exam, but does not request any future action\n",
    "\n",
    "JSON: {{\"follow_up\": true/false, \"confidence\": \"high\"/\"low\", \"snippet\": \"text\"}}\n",
    "\n",
    "If follow_up=false, snippet=\"\"\n",
    "Read FINDINGS first to check if templates apply\n",
    "\n",
    "Report:\n",
    "{report_text}\"\"\"\n",
    "\n",
    "def classify_report(row):\n",
    "    \"\"\"Classify single report via Ollama API\"\"\"\n",
    "    try:\n",
    "        prompt = PROMPT_TEMPLATE.format(report_text=row.report_text)\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_URL}/api/generate\",\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"temperature\": 0,\n",
    "                \"stream\": False,\n",
    "                \"thinking\": \"low\",\n",
    "                \"options\": {\n",
    "                    \"num_predict\": 2000,  # Prevent JSON truncation\n",
    "                    \"num_ctx\": 4096\n",
    "                }\n",
    "            },\n",
    "            timeout=120\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"HTTP {response.status_code}: {response.text}\")\n",
    "        \n",
    "        resp_text = response.json()[\"response\"].strip()\n",
    "\n",
    "        # Try to parse JSON\n",
    "        result = {\n",
    "            \"follow_up\": False,\n",
    "            \"confidence\": \"low\",\n",
    "            \"snippet\": \"\",\n",
    "        }\n",
    "        try:\n",
    "            obj = json.loads(resp_text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Attempt to remove code block fencing that some models add\n",
    "            if resp_text.startswith(\"```json\\n\"):\n",
    "                resp_text = resp_text[len(\"```json\\n\"):]\n",
    "            if resp_text.endswith(\"```\"):\n",
    "                resp_text = resp_text[:-len(\"```\")]\n",
    "            try:\n",
    "                obj = json.loads(resp_text)\n",
    "            except json.JSONDecodeError:\n",
    "                # If still can't parse, return defaults with error\n",
    "                raise Exception(f\"Failed to parse JSON response: {resp_text}\")\n",
    "    \n",
    "        # Update result with parsed values\n",
    "        if isinstance(obj, dict):\n",
    "            result[\"follow_up\"] = bool(obj.get(\"follow_up\", False))\n",
    "            result[\"confidence\"] = str(obj.get(\"confidence\", \"low\")).lower()\n",
    "            result[\"snippet\"] = str(obj.get(\"snippet\", \"\"))\n",
    "    \n",
    "        # Normalize confidence to \"high\" or \"low\"\n",
    "        if result[\"confidence\"] not in [\"high\", \"low\"]:\n",
    "            result[\"confidence\"] = \"low\"\n",
    "        \n",
    "        return {\n",
    "            \"message_control_id\": row.message_control_id,\n",
    "            \"followup_detected\": bool(result.get(\"follow_up\", False)),\n",
    "            \"followup_confidence\": result.get(\"confidence\", \"low\"),\n",
    "            \"followup_snippet\": str(result.get(\"snippet\", \"\")),\n",
    "            \"error\": None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"message_control_id\": row.message_control_id,\n",
    "            \"followup_detected\": None,\n",
    "            \"followup_confidence\": None,\n",
    "            \"followup_snippet\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "print(\"âœ… Classification function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vqe9nno5ot",
   "metadata": {},
   "source": [
    "## TEST RUN: Process 100 Sample Reports\n",
    "\n",
    "Run this cell to test the pipeline on a small sample before processing all reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j27xz2b84mg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST RUN: Process 100 sample reports\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"ðŸ§ª TEST RUN: Processing 100 sample reports...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load model into memory\n",
    "print(\"\\nðŸ“¥ Loading model into memory...\")\n",
    "resp = requests.post(f\"{OLLAMA_URL}/api/generate\", json={\"model\": MODEL, \"keep_alive\": -1})\n",
    "if resp.status_code == 200:\n",
    "    print(\"âœ… Model loaded into memory\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Warning: Failed to load model: {resp.text}\")\n",
    "\n",
    "# Get 100 unprocessed reports for testing\n",
    "test_count = (spark.table(DEST_TABLE)\n",
    "             .filter(F.col(\"followup_processed_at\").isNull())\n",
    "             .count())\n",
    "\n",
    "if test_count == 0:\n",
    "    print(\"âš ï¸  No unprocessed reports found for testing!\")\n",
    "else:\n",
    "    print(f\"\\nðŸ“Š Test sample: 100 reports (out of {test_count:,} unprocessed)\")\n",
    "    \n",
    "    # Get 100 reports as Pandas DataFrame\n",
    "    df_test = (spark.table(DEST_TABLE)\n",
    "              .filter(F.col(\"followup_processed_at\").isNull())\n",
    "              .limit(100)\n",
    "              .select(\"message_control_id\", \"report_text\")\n",
    "              .toPandas())\n",
    "    \n",
    "    # Time the processing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process with simple ThreadPoolExecutor\n",
    "    print(\"\\nâ³ Processing with ThreadPoolExecutor...\")\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=TOTAL_WORKERS) as executor:\n",
    "        futures = {executor.submit(classify_report, row): idx \n",
    "                  for idx, row in df_test.iterrows()}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    result_schema = StructType([\n",
    "        StructField(\"message_control_id\", StringType(), True),\n",
    "        StructField(\"followup_detected\", BooleanType(), True),\n",
    "        StructField(\"followup_confidence\", StringType(), True),\n",
    "        StructField(\"followup_snippet\", StringType(), True),\n",
    "        StructField(\"error\", StringType(), True)\n",
    "    ])\n",
    "    df_results = spark.createDataFrame(results, schema=result_schema)\n",
    "    \n",
    "    # Separate successes and failures\n",
    "    df_success = df_results.filter(F.col(\"error\").isNull()).drop(\"error\")\n",
    "    df_errors = df_results.filter(F.col(\"error\").isNotNull())\n",
    "    \n",
    "    success_count = df_success.count()\n",
    "    error_count = df_errors.count()\n",
    "    total_processed = success_count + error_count\n",
    "    \n",
    "    print(f\"\\nâœ… Test complete!\")\n",
    "    print(f\"\\nðŸ“ˆ Results:\")\n",
    "    print(f\"  Processed: {success_count} reports\")\n",
    "    print(f\"  Errors: {error_count} reports\")\n",
    "    print(f\"  Time: {elapsed:.1f} seconds ({elapsed/max(1,total_processed):.2f}s per report)\")\n",
    "    print(f\"  Throughput: {total_processed/max(1,elapsed):.2f} reports/second\")\n",
    "    \n",
    "    # Show results breakdown\n",
    "    if success_count > 0:\n",
    "        print(f\"\\nðŸ“Š Follow-up Detection Breakdown:\")\n",
    "        df_success.groupBy(\"followup_detected\", \"followup_confidence\").count().orderBy(\"followup_detected\", \"followup_confidence\").show()\n",
    "        \n",
    "        # Show sample results\n",
    "        print(f\"\\nðŸ” Sample Positive Classifications:\")\n",
    "        df_success.filter(F.col(\"followup_detected\") == True).select(\"message_control_id\", \"followup_confidence\", \"followup_snippet\").limit(5).show(truncate=80)\n",
    "        \n",
    "        print(f\"\\nðŸ” Sample Negative Classifications:\")\n",
    "        df_success.filter(F.col(\"followup_detected\") == False).select(\"message_control_id\", \"followup_confidence\", \"followup_snippet\").limit(5).show(truncate=80)\n",
    "    \n",
    "    # Show errors if any\n",
    "    if error_count > 0:\n",
    "        print(f\"\\nâš ï¸  Errors encountered:\")\n",
    "        df_errors.select(\"message_control_id\", \"error\").limit(10).show(truncate=80)\n",
    "    \n",
    "    # Log errors to error table\n",
    "    if error_count > 0:\n",
    "        (df_errors\n",
    "         .select(\n",
    "             F.col(\"message_control_id\"),\n",
    "             F.col(\"error\"),\n",
    "             F.current_timestamp().alias(\"error_timestamp\")\n",
    "         )\n",
    "         .write\n",
    "         .format(\"delta\")\n",
    "         .mode(\"append\")\n",
    "         .saveAsTable(ERROR_TABLE))\n",
    "        print(f\"  âš ï¸  {error_count} errors logged to {ERROR_TABLE}\")\n",
    "    \n",
    "    # Write successful results to table\n",
    "    if success_count > 0:\n",
    "        df_success.createOrReplaceTempView(\"test_results\")\n",
    "        spark.sql(f\"\"\"\n",
    "            MERGE INTO {DEST_TABLE} AS target\n",
    "            USING test_results AS source\n",
    "            ON target.message_control_id = source.message_control_id\n",
    "            WHEN MATCHED THEN UPDATE SET\n",
    "                target.followup_detected = source.followup_detected,\n",
    "                target.followup_confidence = source.followup_confidence,\n",
    "                target.followup_snippet = source.followup_snippet,\n",
    "                target.followup_processed_at = current_timestamp()\n",
    "        \"\"\")\n",
    "        print(f\"  âœ… {success_count} results written to {DEST_TABLE}\")\n",
    "    \n",
    "    # Estimate full processing time\n",
    "    total_unprocessed = spark.table(DEST_TABLE).filter(F.col(\"followup_processed_at\").isNull()).count()\n",
    "    if total_processed > 0:\n",
    "        estimated_hours = (total_unprocessed * (elapsed / total_processed)) / 3600\n",
    "        print(f\"\\nâ±ï¸  Estimated time for {total_unprocessed:,} remaining reports: {estimated_hours:.1f} hours ({estimated_hours/24:.1f} days)\")\n",
    "        print(f\"   Based on {elapsed/total_processed:.2f}s per report from this test run\")\n",
    "\n",
    "# Unload model from memory\n",
    "#print(\"\\nðŸ“¤ Unloading model from memory...\")\n",
    "#requests.post(f\"{OLLAMA_URL}/api/generate\", json={\"model\": MODEL, \"keep_alive\": 0})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Test run complete! Review results above before running full pipeline.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b59f06-ad30-4e0f-a572-2499de172990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from followup_review_dashboard import create_review_dashboard\n",
    "\n",
    "# df_test is already a Pandas DataFrame with message_control_id and report_text\n",
    "# Just merge it with the success results\n",
    "df_review = df_success.toPandas().merge(\n",
    "  df_test[[\"message_control_id\", \"report_text\"]],\n",
    "  on=\"message_control_id\",\n",
    "  how=\"left\"\n",
    ")\n",
    "\n",
    "# Create the dashboard\n",
    "create_review_dashboard(df_review, report_col='report_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Cell 6: Process Reports with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": "# Import parallel processing module\nfrom parallel_processor import process_reports_in_batches\n\n# Process all reports using simple ThreadPoolExecutor approach\n# Priority filter processes keyword matches first\nstats = process_reports_in_batches(\n    spark=spark,\n    classify_func=classify_report,\n    dest_table=DEST_TABLE,\n    error_table=ERROR_TABLE,\n    ollama_url=OLLAMA_URL,\n    model=MODEL,\n    total_workers=TOTAL_WORKERS,\n    batch_size=BATCH_SIZE,\n    priority_filter=PRIORITY_FILTER  # Process keyword matches first\n)\n\nprint(\"\\nâœ… Processing complete!\")\nprint(f\"\\nðŸ“Š Final Statistics:\")\nprint(f\"  Total processed: {stats['total_processed']:,}\")\nprint(f\"  Total errors: {stats['total_errors']:,}\")\nprint(f\"  Total time: {stats['elapsed_time']/3600:.2f} hours\")"
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Cell 7: Validation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": "# Summary statistics\ndf_final = spark.table(DEST_TABLE)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸ“Š FOLLOW-UP DETECTION SUMMARY\")\nprint(\"=\"*60)\n\nprint(\"\\nResults by follow-up status:\")\ndf_final.groupBy(\"followup_detected\").count().orderBy(\"followup_detected\").show()\n\nprint(\"\\nResults by follow-up status and confidence:\")\ndf_final.groupBy(\"followup_detected\", \"followup_confidence\").count().orderBy(\"followup_detected\", \"followup_confidence\").show()\n\nprint(\"\\nProcessing statistics:\")\nprocessed = df_final.filter(F.col(\"followup_processed_at\").isNotNull()).count()\ntotal_reports = df_final.count()\nprint(f\"  Total reports: {total_reports:,}\")\nprint(f\"  Processed: {processed:,} ({100*processed/total_reports:.1f}%)\")\nprint(f\"  Unprocessed: {total_reports - processed:,}\")\n\nfollowup_count = df_final.filter(F.col(\"followup_detected\") == True).count()\nif processed > 0:\n    print(f\"  Follow-up rate: {100*followup_count/processed:.1f}%\")\n\nfollowup_high_conf = df_final.filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"high\")).count()\nif followup_count > 0:\n    print(f\"  High confidence follow-ups: {followup_high_conf:,} ({100*followup_high_conf/followup_count:.1f}%)\")\n\nprint(\"\\nðŸ” Sample High Confidence Follow-up Reports:\")\n(df_final\n    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"high\"))\n    .select(\"message_control_id\", \"followup_confidence\", \"followup_snippet\")\n    .limit(10)\n    .show(truncate=80)\n)\n\nprint(\"\\nðŸ” Sample Low Confidence Follow-up Reports:\")\n(df_final\n    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"low\"))\n    .select(\"message_control_id\", \"followup_confidence\", \"followup_snippet\")\n    .limit(10)\n    .show(truncate=80)\n)\n\n# Check errors\ndf_errors = spark.table(ERROR_TABLE)\nerror_count = df_errors.count()\nprint(f\"\\nâš ï¸  Error Statistics:\")\nprint(f\"  Total errors: {error_count:,}\")\nif error_count > 0:\n    print(f\"  Error rate: {100*error_count/(processed+error_count):.2f}%\")\n    print(\"\\n  Top error types:\")\n    df_errors.groupBy(\"error\").count().orderBy(F.col(\"count\").desc()).limit(5).show(truncate=80)\n\n# Export samples for validation\nprint(\"\\nðŸ’¾ Exporting validation samples...\")\noutput_dir = \"/home/jovyan/followup_validation\"\nos.makedirs(output_dir, exist_ok=True)\n\n# High confidence positive samples\nhigh_conf_sample = (df_final\n    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"high\"))\n    .select(\"message_control_id\", \"followup_detected\", \"followup_confidence\", \"followup_snippet\", \"report_text\")\n    .limit(100)\n    .toPandas())\nhigh_conf_sample.to_csv(f\"{output_dir}/followup_high_confidence_sample.csv\", index=False)\nprint(f\"  âœ… Saved: {output_dir}/followup_high_confidence_sample.csv\")\n\n# Low confidence positive samples\nlow_conf_sample = (df_final\n    .filter((F.col(\"followup_detected\") == True) & (F.col(\"followup_confidence\") == \"low\"))\n    .select(\"message_control_id\", \"followup_detected\", \"followup_confidence\", \"followup_snippet\", \"report_text\")\n    .limit(100)\n    .toPandas())\nlow_conf_sample.to_csv(f\"{output_dir}/followup_low_confidence_sample.csv\", index=False)\nprint(f\"  âœ… Saved: {output_dir}/followup_low_confidence_sample.csv\")\n\n# Negative samples\nnegative_sample = (df_final\n    .filter(F.col(\"followup_detected\") == False)\n    .select(\"message_control_id\", \"followup_detected\", \"followup_confidence\", \"followup_snippet\", \"report_text\")\n    .limit(100)\n    .toPandas())\nnegative_sample.to_csv(f\"{output_dir}/followup_negative_sample.csv\", index=False)\nprint(f\"  âœ… Saved: {output_dir}/followup_negative_sample.csv\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸŽ‰ PROCESSING COMPLETE\")\nprint(\"=\"*60)\n\n# Interactive Dashboard Review\nprint(\"\\n\" + \"=\"*60)\nprint(\"ðŸ“± INTERACTIVE DASHBOARD REVIEW\")\nprint(\"=\"*60)\n\nfrom followup_review_dashboard import create_review_dashboard\n\n# Stratified sample across modalities AND classifications\ndf_processed = df_final.filter(F.col(\"followup_processed_at\").isNotNull())\n\nprint(\"\\nðŸ”„ Creating stratified sample by modality...\")\n# Get 10 samples per modality per classification type\ndf_review = (df_processed\n    .filter(F.col(\"modality\").isNotNull())\n    .withColumn(\"category\", F.concat_ws(\"_\", \n                                        F.col(\"modality\"),\n                                        F.col(\"followup_detected\").cast(\"string\"),\n                                        F.coalesce(F.col(\"followup_confidence\"), F.lit(\"null\"))))\n    .withColumn(\"row_num\", F.row_number().over(\n        Window.partitionBy(\"category\").orderBy(F.rand())\n    ))\n    .filter(F.col(\"row_num\") <= 10)  # 10 per category\n    .select(\"message_control_id\", \"modality\", \"followup_detected\", \n            \"followup_confidence\", \"followup_snippet\", \"report_text\")\n    .toPandas())\n\nprint(f\"\\nðŸ“Š Loaded {len(df_review)} reports stratified by modality\")\nprint(\"\\nSample distribution:\")\nprint(df_review.groupby(['modality', 'followup_detected', 'followup_confidence']).size())\n\n# Launch dashboard\nprint(\"\\nðŸš€ Launching interactive dashboard...\")\ncreate_review_dashboard(df_review, report_col='report_text')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}